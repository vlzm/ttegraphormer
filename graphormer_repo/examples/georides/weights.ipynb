{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a861a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abb24daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_omsk = osp.join('omsk', 'ckpts', 'checkpoint_last.pt')\n",
    "model_state_omsk = torch.load(checkpoint_path_omsk)[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90533023",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_abakan = osp.join('ckpts', 'checkpoint_last.pt')\n",
    "model_state_abakan = torch.load(checkpoint_path_abakan)[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "354b0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_omsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "caf1c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_abakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "004b4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"graphormer_omsk.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model_state_omsk, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4a93cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"graphormer_abakan.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model_state_abakan.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ada57634",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"graphormer_omsk.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a738186",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14360/3968625051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_hasattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_graphormer_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphormerGraphEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from fairseq import utils\n",
    "from fairseq.models import (\n",
    "    FairseqEncoder,\n",
    "    FairseqEncoderModel,\n",
    "    register_model,\n",
    "    register_model_architecture,\n",
    ")\n",
    "from fairseq.modules import (\n",
    "    LayerNorm,\n",
    ")\n",
    "from fairseq.utils import safe_hasattr\n",
    "\n",
    "from ..modules import init_graphormer_params, GraphormerGraphEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from ..pretrain import load_pretrained_model\n",
    "import time\n",
    "\n",
    "@register_model(\"graphormer\")\n",
    "class GraphormerModel(FairseqEncoderModel):\n",
    "    def __init__(self, args, encoder):\n",
    "        super().__init__(encoder)\n",
    "        self.args = args\n",
    "\n",
    "        if getattr(args, \"apply_graphormer_init\", False):\n",
    "            self.apply(init_graphormer_params)\n",
    "        self.encoder_embed_dim = args.encoder_embed_dim\n",
    "        if args.pretrained_model_name != \"none\":\n",
    "            self.load_state_dict(load_pretrained_model(args.pretrained_model_name))\n",
    "            if not args.load_pretrained_model_output_layer:\n",
    "                self.encoder.reset_output_layer_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add model-specific arguments to the parser.\"\"\"\n",
    "        # Arguments related to dropout\n",
    "        parser.add_argument(\n",
    "            \"--dropout\", type=float, metavar=\"D\", help=\"dropout probability\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--attention-dropout\",\n",
    "            type=float,\n",
    "            metavar=\"D\",\n",
    "            help=\"dropout probability for\" \" attention weights\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--act-dropout\",\n",
    "            type=float,\n",
    "            metavar=\"D\",\n",
    "            help=\"dropout probability after\" \" activation in FFN\",\n",
    "        )\n",
    "\n",
    "        # Arguments related to hidden states and self-attention\n",
    "        parser.add_argument(\n",
    "            \"--encoder-ffn-embed-dim\",\n",
    "            type=int,\n",
    "            metavar=\"N\",\n",
    "            help=\"encoder embedding dimension for FFN\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder-layers\", type=int, metavar=\"N\", help=\"num encoder layers\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder-attention-heads\",\n",
    "            type=int,\n",
    "            metavar=\"N\",\n",
    "            help=\"num encoder attention heads\",\n",
    "        )\n",
    "\n",
    "        # Arguments related to input and output embeddings\n",
    "        parser.add_argument(\n",
    "            \"--encoder-embed-dim\",\n",
    "            type=int,\n",
    "            metavar=\"N\",\n",
    "            help=\"encoder embedding dimension\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--share-encoder-input-output-embed\",\n",
    "            action=\"store_true\",\n",
    "            help=\"share encoder input\" \" and output embeddings\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder-learned-pos\",\n",
    "            action=\"store_true\",\n",
    "            help=\"use learned positional embeddings in the encoder\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--no-token-positional-embeddings\",\n",
    "            action=\"store_true\",\n",
    "            help=\"if set, disables positional embeddings\" \" (outside self attention)\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--max-positions\", type=int, help=\"number of positional embeddings to learn\"\n",
    "        )\n",
    "\n",
    "        # Arguments related to parameter initialization\n",
    "        parser.add_argument(\n",
    "            \"--apply-graphormer-init\",\n",
    "            action=\"store_true\",\n",
    "            help=\"use custom param initialization for Graphormer\",\n",
    "        )\n",
    "\n",
    "        # misc params\n",
    "        parser.add_argument(\n",
    "            \"--activation-fn\",\n",
    "            choices=utils.get_available_activation_fns(),\n",
    "            help=\"activation function to use\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--encoder-normalize-before\",\n",
    "            action=\"store_true\",\n",
    "            help=\"apply layernorm before each encoder block\",\n",
    "        )\n",
    "\n",
    "    def max_nodes(self):\n",
    "        return self.encoder.max_nodes\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, args, task):\n",
    "        \"\"\"Build a new model instance.\"\"\"\n",
    "        # make sure all arguments are present in older models\n",
    "        base_architecture(args)\n",
    "\n",
    "        if not safe_hasattr(args, \"max_nodes\"):\n",
    "            args.max_nodes = args.tokens_per_sample\n",
    "\n",
    "        logger.info(args)\n",
    "\n",
    "        encoder = GraphormerEncoder(args)\n",
    "        return cls(args, encoder)\n",
    "\n",
    "    def forward(self, batched_data, **kwargs):\n",
    "        return self.encoder(batched_data, **kwargs)\n",
    "\n",
    "\n",
    "class GraphormerEncoder(FairseqEncoder):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(dictionary=None)\n",
    "        self.max_nodes = args.max_nodes\n",
    "\n",
    "        self.graph_encoder = GraphormerGraphEncoder(\n",
    "            # < for graphormer\n",
    "            num_atoms=args.num_atoms,\n",
    "            num_in_degree=args.num_in_degree,\n",
    "            num_out_degree=args.num_out_degree,\n",
    "            num_edges=args.num_edges,\n",
    "            num_spatial=args.num_spatial,\n",
    "            num_edge_dis=args.num_edge_dis,\n",
    "            edge_type=args.edge_type,\n",
    "            multi_hop_max_dist=args.multi_hop_max_dist,\n",
    "            # >\n",
    "            num_encoder_layers=args.encoder_layers,\n",
    "            embedding_dim=args.encoder_embed_dim,\n",
    "            ffn_embedding_dim=args.encoder_ffn_embed_dim,\n",
    "            num_attention_heads=args.encoder_attention_heads,\n",
    "            dropout=args.dropout,\n",
    "            attention_dropout=args.attention_dropout,\n",
    "            activation_dropout=args.act_dropout,\n",
    "            encoder_normalize_before=args.encoder_normalize_before,\n",
    "            apply_graphormer_init=args.apply_graphormer_init,\n",
    "            activation_fn=args.activation_fn,\n",
    "        )\n",
    "\n",
    "        self.share_input_output_embed = args.share_encoder_input_output_embed\n",
    "        self.embed_out = None\n",
    "        self.lm_output_learned_bias = None\n",
    "\n",
    "        # Remove head is set to true during fine-tuning\n",
    "        self.load_softmax = not getattr(args, \"remove_head\", False)\n",
    "\n",
    "        self.masked_lm_pooler = nn.Linear(\n",
    "            args.encoder_embed_dim, args.encoder_embed_dim\n",
    "        )\n",
    "\n",
    "        self.lm_head_transform_weight = nn.Linear(\n",
    "            args.encoder_embed_dim, args.encoder_embed_dim\n",
    "        )\n",
    "        self.activation_fn = utils.get_activation_fn(args.activation_fn)\n",
    "        self.layer_norm = LayerNorm(args.encoder_embed_dim)\n",
    "\n",
    "        self.lm_output_learned_bias = None\n",
    "        if self.load_softmax:\n",
    "            self.lm_output_learned_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "            if not self.share_input_output_embed:\n",
    "                self.embed_out = nn.Linear(\n",
    "                    args.encoder_embed_dim, args.num_classes, bias=False\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def reset_output_layer_parameters(self):\n",
    "        self.lm_output_learned_bias = nn.Parameter(torch.zeros(1))\n",
    "        if self.embed_out is not None:\n",
    "            self.embed_out.reset_parameters()\n",
    "\n",
    "    def forward(self, batched_data, perturb=None, masked_tokens=None, **unused):\n",
    "        # print('batched_data zzz:', batched_data)\n",
    "        # time.sleep(20)\n",
    "        inner_states, graph_rep = self.graph_encoder(\n",
    "            batched_data,\n",
    "            perturb=perturb,\n",
    "        )\n",
    "        # print('inner_states', len(inner_states))\n",
    "        # print('inner_states 0', inner_states[0].size())\n",
    "        # print('graph_rep', graph_rep.size())\n",
    "        \n",
    "        x = inner_states[-1].transpose(0, 1)\n",
    "        # print('x 1', x.size())\n",
    "        # project masked tokens only\n",
    "        if masked_tokens is not None:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        x = self.layer_norm(self.activation_fn(self.lm_head_transform_weight(x)))\n",
    "        # print('x 2', x.size())\n",
    "        \n",
    "        # project back to size of vocabulary\n",
    "        if self.share_input_output_embed and hasattr(\n",
    "            self.graph_encoder.embed_tokens, \"weight\"\n",
    "        ):\n",
    "            x = F.linear(x, self.graph_encoder.embed_tokens.weight)\n",
    "            # print('x 3', x.size())\n",
    "        elif self.embed_out is not None:\n",
    "            x = self.embed_out(x)\n",
    "            # print('x 4', x.size())\n",
    "        if self.lm_output_learned_bias is not None:\n",
    "            x = x + self.lm_output_learned_bias\n",
    "            # print('x 5', x.size())\n",
    "        \n",
    "        # time.sleep(10)\n",
    "        return x\n",
    "\n",
    "    def max_nodes(self):\n",
    "        \"\"\"Maximum output length supported by the encoder.\"\"\"\n",
    "        return self.max_nodes\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        if not self.load_softmax:\n",
    "            for k in list(state_dict.keys()):\n",
    "                if \"embed_out.weight\" in k or \"lm_output_learned_bias\" in k:\n",
    "                    del state_dict[k]\n",
    "        return state_dict\n",
    "\n",
    "\n",
    "@register_model_architecture(\"graphormer\", \"graphormer\")\n",
    "def base_architecture(args):\n",
    "    args.dropout = getattr(args, \"dropout\", 0.1)\n",
    "    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n",
    "    args.act_dropout = getattr(args, \"act_dropout\", 0.0)\n",
    "\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 4096)\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 6)\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 8)\n",
    "\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 1024)\n",
    "    args.share_encoder_input_output_embed = getattr(\n",
    "        args, \"share_encoder_input_output_embed\", False\n",
    "    )\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "\n",
    "    args.apply_graphormer_init = getattr(args, \"apply_graphormer_init\", False)\n",
    "\n",
    "    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.encoder_normalize_before = getattr(args, \"encoder_normalize_before\", True)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"graphormer\", \"graphormer_base\")\n",
    "def graphormer_base_architecture(args):\n",
    "    if args.pretrained_model_name == \"pcqm4mv1_graphormer_base\":\n",
    "        args.encoder_layers = 12\n",
    "        args.encoder_attention_heads = 32\n",
    "        args.encoder_ffn_embed_dim = 768\n",
    "        args.encoder_embed_dim = 768\n",
    "        args.dropout = getattr(args, \"dropout\", 0.0)\n",
    "        args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n",
    "        args.act_dropout = getattr(args, \"act_dropout\", 0.1)\n",
    "    elif args.pretrained_model_name == \"pcqm4mv2_graphormer_base\":\n",
    "        args.encoder_layers = 12\n",
    "        args.encoder_attention_heads = 32\n",
    "        args.encoder_ffn_embed_dim = 768\n",
    "        args.encoder_embed_dim = 768\n",
    "        args.dropout = getattr(args, \"dropout\", 0.0)\n",
    "        args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n",
    "        args.act_dropout = getattr(args, \"act_dropout\", 0.1)\n",
    "    else:\n",
    "        args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 768)\n",
    "\n",
    "        args.encoder_layers = getattr(args, \"encoder_layers\", 12)\n",
    "\n",
    "        args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 32)\n",
    "        args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 768)\n",
    "\n",
    "    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.encoder_normalize_before = getattr(args, \"encoder_normalize_before\", True)\n",
    "    args.apply_graphormer_init = getattr(args, \"apply_graphormer_init\", True)\n",
    "    args.share_encoder_input_output_embed = getattr(\n",
    "            args, \"share_encoder_input_output_embed\", False\n",
    "        )\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"graphormer\", \"graphormer_slim\")\n",
    "def graphormer_slim_architecture(args):\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 80)\n",
    "\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 12)\n",
    "\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 8)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 80)\n",
    "\n",
    "    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.encoder_normalize_before = getattr(args, \"encoder_normalize_before\", True)\n",
    "    args.apply_graphormer_init = getattr(args, \"apply_graphormer_init\", True)\n",
    "    args.share_encoder_input_output_embed = getattr(\n",
    "            args, \"share_encoder_input_output_embed\", False\n",
    "        )\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "    base_architecture(args)\n",
    "\n",
    "\n",
    "@register_model_architecture(\"graphormer\", \"graphormer_large\")\n",
    "def graphormer_large_architecture(args):\n",
    "    args.encoder_embed_dim = getattr(args, \"encoder_embed_dim\", 1024)\n",
    "\n",
    "    args.encoder_layers = getattr(args, \"encoder_layers\", 24)\n",
    "\n",
    "    args.encoder_attention_heads = getattr(args, \"encoder_attention_heads\", 32)\n",
    "    args.encoder_ffn_embed_dim = getattr(args, \"encoder_ffn_embed_dim\", 1024)\n",
    "\n",
    "    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n",
    "    args.encoder_normalize_before = getattr(args, \"encoder_normalize_before\", True)\n",
    "    args.apply_graphormer_init = getattr(args, \"apply_graphormer_init\", True)\n",
    "    args.share_encoder_input_output_embed = getattr(\n",
    "            args, \"share_encoder_input_output_embed\", False\n",
    "        )\n",
    "    args.no_token_positional_embeddings = getattr(\n",
    "        args, \"no_token_positional_embeddings\", False\n",
    "    )\n",
    "    base_architecture(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb037fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphormer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14360/4229761346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphormer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'graphormer' is not defined"
     ]
    }
   ],
   "source": [
    "model = graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de10e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b373ee-f9db-4289-b470-046c2a3879cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from fairseq import checkpoint_utils, utils, options, tasks\n",
    "from fairseq.logging import progress_bar\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "import ogb\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "\n",
    "from os import path\n",
    "# sys.path.append( path.dirname( path.dirname( path.abspath(__file__) ) ) )\n",
    "import logging\n",
    "from data_class import geo_Omsk, single_geo_Omsk, GraphormerPYGDataset_predict, single_geo_Abakan\n",
    "import os.path as osp\n",
    "from torch_geometric.data import Dataset\n",
    "from functools import lru_cache\n",
    "import torch_geometric.datasets\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from ogb.lsc.pcqm4m_pyg import PygPCQM4MDataset\n",
    "import pyximport\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "pyximport.install(setup_args={'include_dirs': np.get_include()})\n",
    "from torch_geometric.data import Data\n",
    "import time\n",
    "from torch_geometric.utils import add_self_loops, negative_sampling\n",
    "import copy\n",
    "from fairseq.data import (\n",
    "    NestedDictionaryDataset,\n",
    "    NumSamplesDataset,\n",
    ")\n",
    "import json\n",
    "import pathlib\n",
    "link = pathlib.Path().resolve()\n",
    "link = str(link).split('TransTTE')[0]\n",
    "GLOBAL_ROOT = link + 'TransTTE'\n",
    "\n",
    "sys.path.insert(2, GLOBAL_ROOT + '/graphormer_repo/graphormer')\n",
    "from data.wrapper import preprocess_item\n",
    "\n",
    "from pretrain import load_pretrained_model\n",
    "from data.pyg_datasets.pyg_dataset import GraphormerPYGDataset\n",
    "from data.dataset import (\n",
    "    BatchedDataDataset,\n",
    "    TargetDataset,\n",
    "    GraphormerDataset)\n",
    "\n",
    "def eval(args, use_pretrained, checkpoint_path=None, logger=None, data_name = None, predict_dataset = None):\n",
    "    cfg = convert_namespace_to_omegaconf(args)\n",
    "    np.random.seed(cfg.common.seed)\n",
    "    utils.set_torch_seed(cfg.common.seed)\n",
    "    seed = 71\n",
    "    \n",
    "    \n",
    "    # ### data loading\n",
    "    # print('START SINGLE')\n",
    "    # root = osp.join(GLOBAL_ROOT, 'datasets', data_name)\n",
    "    # if data_name == 'omsk':\n",
    "    #     raw_dir = osp.join(root, 'processed', 'data_omsk_1')\n",
    "    #     data = single_geo_Omsk(root = raw_dir)\n",
    "    # if data_name == 'abakan':\n",
    "    #     data = single_geo_Abakan(predict_dataset)\n",
    "    # print('END SINGLE')\n",
    "    \n",
    "    print('1')\n",
    "    GPYG = GraphormerPYGDataset_predict(predict_dataset, seed, None, data, data_name)\n",
    "    print('2')\n",
    "    batched_data = BatchedDataDataset(GPYG)\n",
    "    print('3')\n",
    "    data_sizes = np.array([128] * len(batched_data))\n",
    "    print('4')\n",
    "    dataset_total = NestedDictionaryDataset(\n",
    "            {\n",
    "                \"nsamples\": NumSamplesDataset(),\n",
    "                \"net_input\": {\"batched_data\": batched_data},\n",
    "                \"target\": batched_data,\n",
    "            },\n",
    "        sizes=data_sizes,\n",
    "        )\n",
    "    ###\n",
    "    print('5')\n",
    "    ### initialize task\n",
    "    task = tasks.setup_task(cfg.task)\n",
    "    print('6')\n",
    "    model = task.build_model(cfg.model)\n",
    "    batch_iterator = task.get_batch_iterator(\n",
    "        dataset=dataset_total\n",
    "    )\n",
    "    itr = batch_iterator.next_epoch_itr(shuffle=False, set_dataset_epoch=False)\n",
    "    progress = progress_bar.progress_bar(itr)\n",
    "    ###\n",
    "    \n",
    "    ### load checkpoint\n",
    "    model_state = torch.load(checkpoint_path)[\"model\"]\n",
    "    model.load_state_dict(model_state, strict=True, model_cfg=cfg.model)\n",
    "    model.to(torch.cuda.current_device())\n",
    "    del model_state\n",
    "    ###\n",
    "    \n",
    "    ### prediction\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, sample in enumerate(progress):\n",
    "            sample = utils.move_to_cuda(sample)\n",
    "            y = model(**sample[\"net_input\"])[:, 0, :].reshape(-1)\n",
    "            y_pred.extend(y.detach().cpu())\n",
    "            torch.cuda.empty_cache()\n",
    "    ###\n",
    "    \n",
    "    # save predictions\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    print(y_pred)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def predict_time(dataset_name, predict_dataset):\n",
    "    \n",
    "    parser_dict = dict()\n",
    "    parser_dict['num-atoms'] = str(6656)\n",
    "    parser_dict['dataset_name'] = dataset_name\n",
    "    train_parser = options.get_training_parser()\n",
    "    train_parser.add_argument(\n",
    "            \"--split\",\n",
    "            type=str,\n",
    "        )\n",
    "    train_parser.add_argument(\n",
    "            \"--metric\",\n",
    "            type=str,\n",
    "        )\n",
    "    train_parser.add_argument(\n",
    "            \"--dataset_name\",\n",
    "            type=str,\n",
    "        )\n",
    "    train_args = options.parse_args_and_arch(\n",
    "        train_parser,\n",
    "        [\n",
    "            '--user-dir' , GLOBAL_ROOT + '/graphormer_repo/graphormer',\n",
    "            '--num-workers' , '10', \n",
    "            '--ddp-backend' , 'legacy_ddp', \n",
    "            '--dataset_name' , parser_dict['dataset_name'], \n",
    "            '--dataset-source' , 'pyg', \n",
    "            '--num-atoms' , parser_dict['num-atoms'], \n",
    "            '--task' , 'graph_prediction', \n",
    "            '--criterion' , 'l1_loss', \n",
    "            '--arch' , 'graphormer_slim',\n",
    "            '--num-classes' , '1', \n",
    "            '--batch-size' , '1', \n",
    "            '--save-dir' ,  GLOBAL_ROOT + '/graphormer_repo/examples/georides/omsk/ckpts/',\n",
    "            '--split' , 'valid', \n",
    "            '--metric' , 'rmse', \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    args = train_args\n",
    "    checkpoint_fname = 'checkpoint_last.pt'\n",
    "    checkpoint_path = Path(args.save_dir) / checkpoint_fname\n",
    "    y_preds = eval(args, False, checkpoint_path, None, args.dataset_name, predict_dataset)\n",
    "    return y_preds\n",
    "\n",
    "def graphormer_predict(pt_start, pt_end, dataset_name):\n",
    "    convert_table_valid = pd.read_csv(GLOBAL_ROOT + '/datasets/' + dataset_name + '/raw/convert_roads_valid.csv').dropna()\n",
    "    convert_table_valid['edge_coord_start'] = convert_table_valid['edge_coord_start'].apply(lambda x: json.loads(x))\n",
    "    convert_table_valid['edge_coord_end'] = convert_table_valid['edge_coord_end'].apply(lambda x: json.loads(x))\n",
    "\n",
    "    point_start = pt_start\n",
    "    point_end = pt_end\n",
    "\n",
    "    convert_table_valid['point_start_N'] = point_start[0]\n",
    "    convert_table_valid['point_start_E'] = point_start[1]\n",
    "    convert_table_valid['point_end_N'] = point_end[0]\n",
    "    convert_table_valid['point_end_E'] = point_end[1]\n",
    "\n",
    "    convert_table_valid['dist_start'] = convert_table_valid.apply(lambda x: (x['edge_coord_start'][0][0] - x['point_start_N'])**2 + (x['edge_coord_start'][0][1] - x['point_start_E'])**2, axis = 1)\n",
    "    convert_table_valid['dist_end'] = convert_table_valid.apply(lambda x: (x['edge_coord_end'][0][0] - x['point_end_N'])**2 + (x['edge_coord_end'][0][1] - x['point_end_E'])**2, axis = 1)\n",
    "    convert_table_valid['dist_mean'] = (convert_table_valid['dist_start'] + convert_table_valid['dist_end'])/2\n",
    "\n",
    "    predict_table = convert_table_valid.sort_values(by = ['dist_mean']).reset_index(drop = True)[:1]\n",
    "\n",
    "    dataset = single_geo_Abakan(predict_table)\n",
    "    dataset = dataset.process()\n",
    "\n",
    "    predicted_time = predict_time(dataset_name, dataset)\n",
    "\n",
    "    return [predict_table['edges_geo'], predicted_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c79a027-aa26-4d1c-919b-2b2254bcbb42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6974/2391962024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpoint_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m91.4237220148\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m53.72369937895\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpoint_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m91.43208882255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m53.726498733\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphormer_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abakan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6974/3019524975.py\u001b[0m in \u001b[0;36mgraphormer_predict\u001b[0;34m(pt_start, pt_end, dataset_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgraphormer_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mconvert_table_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOBAL_ROOT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/datasets/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/raw/convert_roads_valid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6974/3019524975.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgraphormer_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mconvert_table_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOBAL_ROOT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/datasets/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/raw/convert_roads_valid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_end'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_table_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge_coord_end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "point_start = [91.4237220148, 53.72369937895]\n",
    "point_end = [91.43208882255, 53.726498733]\n",
    "a = graphormer_predict(point_start, point_end, 'abakan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe7823-b7b2-47ab-8582-ff764b335926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf30073-f7dd-440a-a090-854db3133949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d4e35-5a84-4237-8a5f-b860e52569c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ab897-fac7-4f11-a5c3-61da8db2e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "План:\n",
    "    1. Разобраться как работает дейкстра, в каком виде он возвращает путь и что это за словарь\n",
    "    Путь это просто лист из точек. Этапред это время\n",
    "    2. Разобраться как действует джаваскрипт который принимает на вход словарь и рисует пути\n",
    "    3. Разобраться как перевести эджи в координаты\n",
    "    4. Сделать функцию, которая на вход принимает две точки и возвращает самый быстрый путь и время (для начала можно просто брать length and width, чтобы отработать скрипт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816ce425-92dd-4f8a-9d19-cea12ec3a88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
